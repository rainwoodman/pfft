<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Tutorial &mdash; PFFT 1.0.8 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="PFFT 1.0.8 documentation" href="index.html" />
    <link rel="next" title="Installation and linking" href="install.html" />
    <link rel="prev" title="Introduction" href="intro.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="install.html" title="Installation and linking"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="intro.html" title="Introduction"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">PFFT 1.0.8 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <p>[2]ifpackageloaded#1#2 [2]ifpackageloaded#1#2 [3]ifpackageloaded#1#2#3</p>
<p>#1</p>
<div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>The following chapter describes the usage of the PFFT library at the
example of a simple test file in the first section, followed by the more
advanced features of PFFT in the next sections.</p>
<div class="section" id="a-first-parallel-transform-three-dimensional-fft-with-two-dimensional-data-decomposition">
<h2>A first parallel transform - Three-dimensional FFT with two-dimensional data decomposition<a class="headerlink" href="#a-first-parallel-transform-three-dimensional-fft-with-two-dimensional-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p>We explain the basic steps for computing a parallel FFT with the PFFT
library at the example of the short test program given by
Listing&nbsp;[lst:man<sub>c</sub>2c]. This test computes a three-dimensional
c2c-FFT on a two-dimensional process mesh. The source code
<code class="docutils literal"><span class="pre">manual_c2c_3d.c</span></code> can be found in directory <code class="docutils literal"><span class="pre">tests/</span></code> of the
library’s source code tree.</p>
<p>After initializing MPI with <code class="docutils literal"><span class="pre">MPI_Init</span></code> and before calling any other
PFFT routine initialize the parallel FFT computations via</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_init(void);
</pre></div>
</div>
<p>MPI introduces the concept of communicators to store all the topological
information of the physical process layout. PFFT requires to be called
on a process mesh that corresponds to a periodic, Cartesian
communicator. We assist the user in creating such a communicator with
the following routine</p>
<div class="highlight-python"><div class="highlight"><pre>int pfft_create_procmesh_2d(
    MPI_Comm comm, int np0, int np1,
    MPI_Comm *comm_cart_2d);
</pre></div>
</div>
<p>This routine uses the processes within the communicator <code class="docutils literal"><span class="pre">comm</span></code> to
create a two-dimensional process grid of size <code class="docutils literal"><span class="pre">np0</span></code> x <code class="docutils literal"><span class="pre">np1</span></code> and
stores it into the Cartesian communicator <code class="docutils literal"><span class="pre">comm_cart_2d</span></code>. Note that
<code class="docutils literal"><span class="pre">comm_cart_2d</span></code> is allocated by the routine and must be freed with
<code class="docutils literal"><span class="pre">MPI_Comm_free</span></code> after usage. The input parameter <code class="docutils literal"><span class="pre">comm</span></code> is a
communicator, indicating which processes will participate in the
transform. Choosing <code class="docutils literal"><span class="pre">comm</span></code> as <code class="docutils literal"><span class="pre">MPI_COMM_WORLD</span></code> implies that the FFT
is computed on all available processes.</p>
<p>At the next step we need to know the data decomposition of the input and
output array, that depends on the array sizes, the process grid and the
chosen parallel algorithm. Therefore, we call</p>
<div class="highlight-python"><div class="highlight"><pre>ptrdiff_t pfft_local_size_3d(
    ptrdiff_t *n, MPI_Comm comm_cart_2d, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
</pre></div>
</div>
<p>Hereby, <code class="docutils literal"><span class="pre">n</span></code>, <code class="docutils literal"><span class="pre">local_ni</span></code>, <code class="docutils literal"><span class="pre">local_i_start</span></code>, <code class="docutils literal"><span class="pre">local_no</span></code>,
<code class="docutils literal"><span class="pre">local_o_start</span></code> are arrays of length <span class="math">\(3\)</span> that must be allocated.
The return value of this function equals the size of the local complex
array that needs to be allocated by every process. In most cases, this
coincides with the product of the local array sizes – but may be bigger,
whenever the parallel algorithm needs some extra storage. The input
value <code class="docutils literal"><span class="pre">n</span></code> gives the three-dimensional FFT size and the flag
<code class="docutils literal"><span class="pre">pfft_flags</span></code> serves to adjust some details of the parallel execution.
For the sake of simplicity, we restrict our self to the case
<code class="docutils literal"><span class="pre">pfft_flags=PFFT_TRANSPOSED_NONE</span></code> for a while and explain the more
sophisticated flags at a later point. The output arrays <code class="docutils literal"><span class="pre">local_ni</span></code> and
<code class="docutils literal"><span class="pre">local_i_start</span></code> give the size and the offset of the local input array
that result from the parallel block distribution of the global input
array, i.e., every process owns the input data <code class="docutils literal"><span class="pre">in[k[0],k[1],k[2]]</span></code>
with <code class="docutils literal"><span class="pre">local_i_start[t]</span> <span class="pre">&lt;=</span> <span class="pre">k[t]</span> <span class="pre">&lt;</span> <span class="pre">local_i_start[t]</span> <span class="pre">``</span>
<span class="pre">local\</span> <span class="pre">:sub:`n`\</span> <span class="pre">i[t]+</span> <span class="pre">for</span> <span class="pre">``t=0,1,2</span></code>. Analogously, the output
parameters <code class="docutils literal"><span class="pre">local_o_start</span></code> and <code class="docutils literal"><span class="pre">local_no</span></code> contain the size and the
offset of the local output array.</p>
<p>Afterward, the input and output arrays must be allocated. Hereby,</p>
<div class="highlight-python"><div class="highlight"><pre>pfft_complex* pfft_alloc_complex(size_t size);
</pre></div>
</div>
<p>is a simple wrapper of <code class="docutils literal"><span class="pre">fftw_alloc_complex</span></code>, which in turn allocates
the memory via <code class="docutils literal"><span class="pre">fftw_malloc</span></code> to ensure proper alignment for SIMD. Have
a look at the FFTW user manual&nbsp; for more details on SIMD memory
alignment and <code class="docutils literal"><span class="pre">fftw_malloc</span></code>. Nevertheless, you can also use any other
dynamic memory allocation.</p>
<p>The planning of a single three-dimensional parallel FFT of size <code class="docutils literal"><span class="pre">n[0]</span></code>
x <code class="docutils literal"><span class="pre">n[1]</span></code> x <code class="docutils literal"><span class="pre">n[2]</span></code> is done by the function</p>
<div class="highlight-python"><div class="highlight"><pre>pfft_plan pfft_plan_dft_3d(
    ptrdiff_t *n, pfft_complex *in, pfft_complex *out,
    MPI_Comm comm_cart_2d, int sign, unsigned pfft_flags);
</pre></div>
</div>
<p>We provide the address of the input and output array by the pointers
<code class="docutils literal"><span class="pre">in</span></code> and <code class="docutils literal"><span class="pre">out</span></code>, respectively. An inplace transform is assumed if
these pointers are equal. The integer <code class="docutils literal"><span class="pre">sign</span></code> gives the sign in the
exponential of the FFT. Possible values are <code class="docutils literal"><span class="pre">PFFT_FORWARD</span></code>
(<span class="math">\(-1\)</span>) and <code class="docutils literal"><span class="pre">PFFT_BACKWARD</span></code> (<span class="math">\(+1\)</span>). Flags passed to the
planner via <code class="docutils literal"><span class="pre">pfft\_flags</span></code> must coincide with the flags that were
passed to <code class="docutils literal"><span class="pre">pfft_local_size_3d</span></code>. Otherwise the data layout of the
parallel execution may not match calculated local array sizes. As return
value we get a PFFT plan, some structure that stores all the information
needed to perform a parallel FFT.</p>
<p>Once the plan is generated, we are allowed to fill the input array
<code class="docutils literal"><span class="pre">in</span></code>. Note, that per default the planning step <code class="docutils literal"><span class="pre">pfft_plan_dft_3d</span></code>
will overwrite input array <code class="docutils literal"><span class="pre">in</span></code>. Therefore, you should not write any
sensitive data into <code class="docutils literal"><span class="pre">in</span></code> until the plan was generated. For simplicity,
our test program makes use of the library function</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_init_input_complex_3d(
    ptrdiff_t *n, ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    pfft_complex *in);
</pre></div>
</div>
<p>to fill the input array with some numbers. Alternatively, one can fill
the array with a function <code class="docutils literal"><span class="pre">func</span></code> of choice and the following loop that
takes account of the parallel data layout</p>
<div class="highlight-python"><div class="highlight"><pre>ptrdiff_t m=0;
for(ptrdiff_t k0=0; k0 &lt; local_ni[0]; k0++)
  for(ptrdiff_t k1=0; k1 &lt; local_ni[1]; k1++)
    for(ptrdiff_t k2=0; k2 &lt; local_ni[2]; k2++)
      in[m++] = func(k0 + local_i_start[0],
                     k1 + local_i_start[1],
                     k2 + local_i_start[2]);
</pre></div>
</div>
<p>The parallel FFT is computed when we execute the generated plan via</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_execute(const pfft_plan plan);
</pre></div>
</div>
<p>Now, the results can be read from <code class="docutils literal"><span class="pre">out</span></code> with an analogous
three-dimensional loop. If we do not want to execute another parallel
FFT of the same type, we free the allocated memory of the plan with</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_destroy_plan(pfft_plan plan);
</pre></div>
</div>
<p>Additionally, we use</p>
<div class="highlight-python"><div class="highlight"><pre>int MPI_Comm_free(MPI_Comm *comm);
</pre></div>
</div>
<p>to free the communicator allocated by <code class="docutils literal"><span class="pre">pfft_create_procmesh_2d</span></code> and</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_free(void *ptr);
</pre></div>
</div>
<p>to free memory allocated by <code class="docutils literal"><span class="pre">pfft_alloc_complex</span></code>. Finally, we exit MPI
via</p>
<div class="highlight-python"><div class="highlight"><pre>int MPI_Finalize(void);
</pre></div>
</div>
</div>
<div class="section" id="porting-fftw-mpi-based-code-to-pfft">
<h2>Porting FFTW-MPI based code to PFFT<a class="headerlink" href="#porting-fftw-mpi-based-code-to-pfft" title="Permalink to this headline">¶</a></h2>
<p>We illustrate the close connection between FFTW-MPI and PFFT at a
three-dimensional MPI example analogous to the example given in the FFTW
manual&nbsp;.</p>
<p>Exactly the same task can be performed with PFFT as given in
Listing&nbsp;[lst:pfft<sub>3</sub>don1d].</p>
<div class="highlight-python"><div class="highlight"><pre>#include &lt;pfft.h&gt;

int main(int argc, char **argv)
{
    const ptrdiff_t n[3] = {..., ..., ...};
    pfft_plan plan;
    pfft_complex *data;
    ptrdiff_t alloc_local, local_ni[3], local_i_start[3], local_no[3], local_o_start[3], i, j, k;
    unsigned pfft_flags = 0;

    MPI_Init(&amp;argc, &amp;argv);
    pfft_init();

    /* get local data size and allocate */
    alloc_local = pfft_local_size_dft_3d(n, MPI_COMM_WORLD, pfft_flags,
                         local_ni, local_i_start,
                         local_no, local_o_start);
    data = pfft_alloc_complex(alloc_local);

    /* create plan for in-place forward DFT */
    plan = pfft_plan_dft_3d(n, data, data, MPI_COMM_WORLD,
                PFFT_FORWARD, PFFT_ESTIMATE);

    /* initialize data to some function my_function(x,y,z) */
    for (i = 0; i &lt; local_n[0]; ++i)
      for (j = 0; j &lt; n[1]; ++j)
        for (k = 0; k &lt; n[2]; ++k)
          data[i*n[1]*n[2] + j*n[2] + k] = my_function(local_i_start[0] + i, j, k);

    /* compute transforms, in-place, as many times as desired */
    pfft_execute(plan);

    pfft_destroy_plan(plan);

    MPI_Finalize();
}
</pre></div>
</div>
<p>substitute <code class="docutils literal"><span class="pre">fftw3-mpi.h</span></code> by <code class="docutils literal"><span class="pre">pfft.h</span></code></p>
<p>substitute all prefixes <code class="docutils literal"><span class="pre">fftw_</span></code> and <code class="docutils literal"><span class="pre">fftw_mpi_</span></code> by <code class="docutils literal"><span class="pre">pfft_</span></code></p>
<p>substitute all prefixes <code class="docutils literal"><span class="pre">FFTW_</span></code> by <code class="docutils literal"><span class="pre">PFFT_</span></code></p>
<p>the integers <code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">local_n0</span></code>, <code class="docutils literal"><span class="pre">local_0_start</span></code> become arrays of
length 3</p>
<p><code class="docutils literal"><span class="pre">dft_</span></code> in <code class="docutils literal"><span class="pre">pfft_local_size_dft_3d</span></code></p>
<p><code class="docutils literal"><span class="pre">pfft_local_size_dft_3d</span></code> has additional input <code class="docutils literal"><span class="pre">pfft_flags</span></code> and
additional outputs <code class="docutils literal"><span class="pre">local_no</span></code>, <code class="docutils literal"><span class="pre">local_o_start</span></code></p>
<p>The loop that inits <code class="docutils literal"><span class="pre">data</span></code> becomes splitted along all three
dimensions. We could also use</p>
<p>First, All prefixes <code class="docutils literal"><span class="pre">fftw_</span></code> are substituted by <code class="docutils literal"><span class="pre">pfft_</span></code></p>
<p>Now, the changes in order to use a two-dimensional process mesh are
marginal as can be seen in Listing&nbsp;[lst:pfft<sub>3</sub>don2d].</p>
<div class="highlight-python"><div class="highlight"><pre>#include &lt;pfft.h&gt;

int main(int argc, char **argv)
{
    const ptrdiff_t n[3] = {..., ..., ...};
    (red@const int np0 = ..., np1 = ...;@*)
    pfft_plan plan;
    pfft_complex *data;
    ptrdiff_t alloc_local, local_ni[3], local_i_start[3], local_no[3], local_o_start[3], i, j, k;
    unsigned pfft_flags = 0;
    (red@MPI_Comm comm_cart_2d;@*)

    MPI_Init(&amp;argc, &amp;argv);
    pfft_init();

    (red@/* create two-dimensional process grid of size np0 x np1 */@*)
    (red@pfft_create_procmesh_2d(MPI_COMM_WORLD, np0, np1,@*)
        (red@&amp;comm_cart_2d);@*)

    /* get local data size and allocate */
    alloc_local = pfft_local_size_dft_3d(n, (red@comm_cart_2d@*), pfft_flags,
                         local_ni, local_i_start,
                         local_no, local_o_start);
    data = pfft_alloc_complex(alloc_local);

    /* create plan for in-place forward DFT */
    plan = pfft_plan_dft_3d(n, data, data, MPI_COMM_WORLD,
                PFFT_FORWARD, PFFT_ESTIMATE);

    /* initialize data to some function my_function(x,y,z) */
    for (i = 0; i &lt; local_n[0]; ++i)
      for (j = 0; j &lt; (red@local_n[1]@*); ++j)
        for (k = 0; k &lt; (red@local_n[2]@*); ++k)
          data[i*(red@local_n[1]*local_n[2]@*) + j*(red@local_n[2]@*) + k] =
              my_function(local_i_start[0] + i,
                  (red@local_i_start[1] +@*) j,
                  (red@local_i_start[2] +@*) k);

    /* compute transforms, in-place, as many times as desired */
    pfft_execute(plan);

    pfft_destroy_plan(plan);

    MPI_Finalize();
}
</pre></div>
</div>
</div>
<div class="section" id="errorcode-for-communicator-creation">
<h2>Errorcode for communicator creation<a class="headerlink" href="#errorcode-for-communicator-creation" title="Permalink to this headline">¶</a></h2>
<p>As we have seen the function</p>
<div class="highlight-python"><div class="highlight"><pre>int pfft_create_procmesh_2d(
    MPI_Comm comm, int np0, int np1,
    MPI_Comm *comm_cart_2d);
</pre></div>
</div>
<p>creates a two-dimensional, periodic, Cartesian communicator. The <code class="docutils literal"><span class="pre">int</span></code>
return value (not used in Listing&nbsp;[lst:man<sub>c</sub>2c]) is the
forwarded error code of <code class="docutils literal"><span class="pre">MPI_Cart_create</span></code>. It is equal to zero if the
communicator was created successfully. The most common error is that the
number of processes within the input communicator <code class="docutils literal"><span class="pre">comm</span></code> does not fit
<code class="docutils literal"><span class="pre">np0</span> <span class="pre">x</span> <span class="pre">np1</span></code>. In this case the Cartesian communicator is not generated
and the return value is unequal to zero. Therefore, a typical sanity
check might look like</p>
<div class="highlight-python"><div class="highlight"><pre>/* Create two-dimensional process grid of size np[0] x np[1],
   if possible */
if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1],
        &amp;comm_cart_2d) )
{
  pfft_fprintf(MPI_COMM_WORLD, stderr,
      &quot;Error: This test file only works with %d processes.\n&quot;,
      np[0]*np[1]);
  MPI_Finalize();
  return 1;
}
</pre></div>
</div>
<p>Hereby, we use the PFFT library function</p>
<div class="highlight-python"><div class="highlight"><pre>void pfft_fprintf(
    MPI_Comm comm, FILE *stream, const char *format, ...);
</pre></div>
</div>
<p>to print the error message. This function is similar to the standard C
function <code class="docutils literal"><span class="pre">fprintf</span></code> with the exception, that only the process with MPI
rank <span class="math">\(0\)</span> within the given communicator <code class="docutils literal"><span class="pre">comm</span></code> will produce some
output; see Section&nbsp;[sec:fprintf] for details.</p>
</div>
<div class="section" id="inplace-transforms">
<h2>Inplace transforms<a class="headerlink" href="#inplace-transforms" title="Permalink to this headline">¶</a></h2>
<p>Similar to FFTW, PFFT is able to compute parallel FFTs completely in
place, which means that beside some constant buffers, no second data
array is necessary. Especially, the global data communication can be
performed in place. As far as we know, there is no other parallel FFT
library beside FFTW and PFFT that supports this feature. This feature is
enabled as soon as the pointer to the output array <code class="docutils literal"><span class="pre">out</span></code> is equal to
the pointer to the input array <code class="docutils literal"><span class="pre">in</span></code>. E.g., in
Listing&nbsp;[lst:man<sub>c</sub>2c] we would call</p>
<div class="highlight-python"><div class="highlight"><pre>/* Plan parallel forward FFT */
plan = pfft_plan_dft_3d(n, in, in, comm_cart_2d,
    PFFT_FORWARD, PFFT_TRANSPOSED_NONE);
</pre></div>
</div>
</div>
<div class="section" id="higher-dimensional-data-decomposition">
<h2>Higher dimensional data decomposition<a class="headerlink" href="#higher-dimensional-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p>The test program given in Listing&nbsp;[lst:man<sub>c</sub>2c] used a
two-dimensional data decomposition of a three-dimensional data set.
Moreover, PFFT support the computation of any <span class="math">\(d\)</span>-dimensional FFT
with <span class="math">\(r\)</span>-dimensional data decomposition as long as
<span class="math">\(r\le d-1\)</span>. For example, one can use a one-dimensional data
decomposition for any two- or higher-dimensional data set, while the
data set must be at least four-dimensional to fit to a three-dimensional
data decomposition. The case <span class="math">\(r=d\)</span> is not supported efficiently,
since during the parallel computations there is always at least one
dimension that remains local, i.e., one dimensions stays non-decomposed.
The only exception from this rule is the case <span class="math">\(d=r=3\)</span> that is
supported by PFFT in a special way, see Section&nbsp;[sec:3don3d] for
details.</p>
<p>The dimensionality of the data decomposition is given by the dimension
of the Cartesian communicator that goes into the PFFT planing routines.
Therefore, we present a generalization of communicator creation function</p>
<div class="highlight-python"><div class="highlight"><pre>int pfft_create_procmesh(
    int rnk_np, MPI_Comm comm, const int *np,
    MPI_Comm *comm_cart);
</pre></div>
</div>
<p>Hereby, the array <code class="docutils literal"><span class="pre">np</span></code> of length <code class="docutils literal"><span class="pre">rnk_np</span></code> gives the size of the
Cartesian communicator <code class="docutils literal"><span class="pre">cart_comm</span></code>.</p>
</div>
<div class="section" id="parallel-data-decomposition">
<h2>Parallel data decomposition<a class="headerlink" href="#parallel-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p>In the following, we use the notation <span class="math">\(\frac{n}{P}\)</span> to symbolize
that an array of length <span class="math">\(n\)</span> is broken into disjoint blocks and
distributed on <span class="math">\(P\)</span> MPI processes. Hereby, the data is distributed
in compliance to the FFTW-MPI data decompostion&nbsp;, i.e., the first
<code class="docutils literal"><span class="pre">P/block</span></code> (rounded down) processes get a contiguous chunk of <code class="docutils literal"><span class="pre">block</span></code>
elements, the next process gets the remaining <code class="docutils literal"><span class="pre">n</span> <span class="pre">-</span> <span class="pre">block</span> <span class="pre">*</span> <span class="pre">(n/block)</span></code>
data elements, and all remaining processes get nothing. Thereby, the
block size <code class="docutils literal"><span class="pre">block</span></code> defaults to <code class="docutils literal"><span class="pre">n/P</span></code> (rounded down) but can also be
user defined.</p>
<div class="section" id="non-transposed-and-transposed-data-layout">
<h3>Non-transposed and transposed data layout<a class="headerlink" href="#non-transposed-and-transposed-data-layout" title="Permalink to this headline">¶</a></h3>
<p>In the following, we use the notation <span class="math">\(\frac{n}{P}\)</span> to symbolize
that an array of length <span class="math">\(n\)</span> is distributed on <span class="math">\(P\)</span> MPI
processes. The standard PFFT data decomposition of <span class="math">\(h\)</span> interleaved
<span class="math">\(d\)</span>-dimensional arrays of equal size
<span class="math">\(n_0 \times n_1\times \dots \times n_{d-1}\)</span> on a
<span class="math">\(r\)</span>-dimensional process mesh of size
<span class="math">\(P_0\times \dots \times P_{r-1}\)</span> is given by the blocks</p>
<div class="math">
\[\frac{n_0}{P_0} \times \frac{n_1}{P_1} \times \dots \times \frac{n_{r-1}}{P_{r-1}}  \times n_r \times n_{r+1} \times \dots \times n_{d-1} \times h.\]</div>
<p>A PFFT created with planning flag <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_NONE</span></code> requires the
inputs to be decomposed in this standard way and produces outputs that
are decomposed in the same way.</p>
<p>PFFT can save half of the global communication amount, if the data
reordering to standard decomposition is omitted. The transposed data
decomposition is given by</p>
<div class="math">
\[\frac{n_1}{P_0} \times \frac{n_2}{P_1} \times \dots \times \frac{n_{r}}{P_{r-1}}  \times n_0 \times n_{r+1} \times \dots \times n_{d-1} \times h\]</div>
<p>A PFFT plan created with planning flag <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_OUT</span></code> produces
outputs with transposed data decomposition. Analogously, a PFFT plan
created with planning flag <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_IN</span></code> requires its inputs to
be decomposed in the transposed way. Typically, one creates a forward
plan with <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_OUT</span></code> and a backward plan with planning flag
<code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_IN</span></code>.</p>
<p>Note that the flags <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_OUT</span></code> and <code class="docutils literal"><span class="pre">PFFT_TRANSPOSED_IN</span></code>
must be passed to the array distribution function (see
Section&nbsp;[sec:local-size]) <em>as well as</em> to the planner (see
Section&nbsp;[sec:create-plan]).</p>
</div>
<div class="section" id="three-dimensional-ffts-with-three-dimensional-data-decomposition">
<h3>Three-dimensional FFTs with three-dimensional data decomposition<a class="headerlink" href="#three-dimensional-ffts-with-three-dimensional-data-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Many applications work with three-dimensional block decompositions of
three-dimensional arrays. PFFT supports decompositions of the kind</p>
<div class="math">
\[\frac{n_0}{P_0} \times \frac{n_1}{P_1} \times \frac{n_2}{P_2} \times h.\]</div>
<p>However, PFFT applies a parallel algorithms that needs at least one
non-distributed transform dimension (we do not transform along
<span class="math">\(h\)</span>), Therefore, we split the number of processes along the last
dimension into two factors <span class="math">\(P_2=Q_1Q_2\)</span>, remap the data to the
two-dimensional decomposition</p>
<div class="math">
\[\frac{n_0}{P_0Q_0} \times \frac{n_1}{P_1Q_1} \times n_2 \times h,\]</div>
<p>and compute the parallel FFT with this two-dimensional decomposition.
Note that the 3d to 2d remap implies some very special restrictions on
the block sizes for <span class="math">\(n_0\)</span> and <span class="math">\(n_1\)</span>, i.e., the blocks must
be divisible by <span class="math">\(Q_0\)</span> and <span class="math">\(Q_1\)</span>. More precisely, the default
blocks of the 2d-decomposition are given by <code class="docutils literal"><span class="pre">n0/(P0*Q0)</span></code> and
<code class="docutils literal"><span class="pre">n1/(P1*Q1)</span></code> (both divisions rounded down). This implies that the
default blocks of the 3d-decomposition must be <code class="docutils literal"><span class="pre">n0/(P0*Q0)</span> <span class="pre">*</span> <span class="pre">Q0</span></code>,
<code class="docutils literal"><span class="pre">n1/(P1*Q1)</span> <span class="pre">*</span> <span class="pre">Q1</span></code>, and <code class="docutils literal"><span class="pre">n2/(Q0*Q1)</span></code> (all divisions rounded down).</p>
</div>
</div>
<div class="section" id="planning-effort">
<h2>Planning effort<a class="headerlink" href="#planning-effort" title="Permalink to this headline">¶</a></h2>
<p>Pass one of the following flags</p>
<p><code class="docutils literal"><span class="pre">PFFT_ESTIMATE</span></code>,</p>
<p><code class="docutils literal"><span class="pre">PFFT_MEASURE</span></code>,</p>
<p><code class="docutils literal"><span class="pre">PFFT_PATIENT</span></code>, or,</p>
<p><code class="docutils literal"><span class="pre">PFFT_EXHAUSIVE</span></code></p>
<p>to the PFFT planner in order to plan all internal FFTW plans with
<code class="docutils literal"><span class="pre">FFTW_ESTIMATE</span></code>, <code class="docutils literal"><span class="pre">FFTW_MEASURE</span></code>, <code class="docutils literal"><span class="pre">FFTW_PATIENT</span></code>, or
<code class="docutils literal"><span class="pre">FFTW_EXHAUSIVE</span></code>, respectively. The default value is <code class="docutils literal"><span class="pre">PFFT_MEASURE</span></code>.</p>
<p>PFFT uses FFTW plans for parallel array transposition and the serial
transforms. In fact, every serial transform is a combination of strided
lower-dimensional FFTs and a serial array transposition (necessary to
prepare the global transposition) which can be done by a single FFTW
plan. However, it turns out that FFTW sometimes performs better if the
serial transposition and the strided FFTs are executed separately.
Therefore, PFFT introduces the flag <code class="docutils literal"><span class="pre">PFFT_TUNE</span></code> that enables extensive
run time tests in order to find the optimal sequence of serial strided
FFT and serial transposition for every serial transform. These tests are
disable on default which corresponds to the flag <code class="docutils literal"><span class="pre">PFFT_NO_TUNE</span></code>.</p>
</div>
<div class="section" id="preserving-input-data">
<h2>Preserving input data<a class="headerlink" href="#preserving-input-data" title="Permalink to this headline">¶</a></h2>
<p>The following flags</p>
<p><code class="docutils literal"><span class="pre">PFFT_PRESERVE_INPUT</span></code>,</p>
<p><code class="docutils literal"><span class="pre">PFFT_DESTROY_INPUT</span></code>, and,</p>
<p><code class="docutils literal"><span class="pre">PFFT_BUFFERED_INPLACE</span></code></p>
<p>only take effect for out-of-place transforms. The first one behaves
analogously to the FFTW flag <code class="docutils literal"><span class="pre">FFTW_PRESERVE_INPUT</span></code> and ensures that
the input values are not overwritten. In fact, this flag implies that
only the first serial transform is executed out-of-place and all
successive steps are performed in-place on the output array. In
compliance to FFTW, this is the default behaviour for out-of-place
plans.</p>
<p>The second flag behaves analogously to the FFTW flag
<code class="docutils literal"><span class="pre">FFTW_DESTROY_INPUT</span></code> and tells the planner that the input array can be
used as scratch array. This may give some speedup for out-of-place
plans, because all the intermediate transforms and transposition steps
can be performed out-of-place.</p>
<p>Finally, the flag <code class="docutils literal"><span class="pre">PFFT_BUFFERED_INPLACE</span></code> can be used for out-of-place
plans that store its inputs and outputs in the same array, i.e., array
<code class="docutils literal"><span class="pre">out</span></code> is used for intermediate out-of-place transforms and
transpositions but the PFFT inputs and outputs are stored in array
<code class="docutils literal"><span class="pre">in</span></code>.</p>
</div>
<div class="section" id="ffts-with-shifted-index-sets">
<h2>FFTs with shifted index sets<a class="headerlink" href="#ffts-with-shifted-index-sets" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">PFFT_SHIFTED_IN</span></code></p>
<p><code class="docutils literal"><span class="pre">PFFT_SHIFTED_OUT</span></code></p>
</div>
<div class="section" id="pruned-fft-and-shifted-index-sets">
<h2>Pruned FFT and Shifted Index Sets<a class="headerlink" href="#pruned-fft-and-shifted-index-sets" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pruned-fft">
<h3>Pruned FFT<a class="headerlink" href="#pruned-fft" title="Permalink to this headline">¶</a></h3>
<p>For pruned r2r- and c2c-FFT are defined as</p>
<div class="math">
\[g_l = \sum_{k=0}^{n_i-1} \hat g_k {\ensuremath{\mathrm{e}^{-2\pi{{\ensuremath{\text{\scriptsize{i}}}}} kl/n}}}, \quad l=0,\dots,n_o-1,\]</div>
<p>where <span class="math">\(n_i\le n\)</span> and <span class="math">\(n_o\le n\)</span>.</p>
</div>
<div class="section" id="shifted-index-sets">
<h3>Shifted Index Sets<a class="headerlink" href="#shifted-index-sets" title="Permalink to this headline">¶</a></h3>
<p>For <span class="math">\(N\in 2{\ensuremath{\mathbb{N}}}\)</span> we define the FFT with
shifted inputs</p>
<p>For <span class="math">\(K,L,N\in 2{\ensuremath{\mathbb{N}}}\)</span>, <span class="math">\(L&lt;N\)</span>,
<span class="math">\(L&lt;N\)</span> we define</p>
</div>
</div>
<div class="section" id="precisions">
<h2>Precisions<a class="headerlink" href="#precisions" title="Permalink to this headline">¶</a></h2>
<p>PFFT handles multiple precisions exactly in the same way as FFTW.
Therefore, we quote part&nbsp; of the FFTW manual in the context of PFFT:</p>
<p>You can install single and long-double precision versions of PFFT, which
replace double with float and long double, respectively; see
[sec:install]. To use these interfaces, you must</p>
<p>Link to the single/long-double libraries; on Unix, <code class="docutils literal"><span class="pre">-lpfftf</span></code> or
<code class="docutils literal"><span class="pre">-lpfftl</span></code> instead of (or in addition to) <code class="docutils literal"><span class="pre">-lpfft</span></code>. (You can link to
the different-precision libraries simultaneously.)</p>
<p>Include the same <code class="docutils literal"><span class="pre">&lt;pfft.h&gt;</span></code> header file.</p>
<p>Replace all lowercase instances of ‘<code class="docutils literal"><span class="pre">pfft_</span></code>’ with ‘<code class="docutils literal"><span class="pre">pfftf_</span></code>’ or
‘<code class="docutils literal"><span class="pre">pfftl_</span></code>’ for single or long-double precision, respectively.
(<code class="docutils literal"><span class="pre">pfft_complex</span></code> becomes <code class="docutils literal"><span class="pre">pfftf_complex</span></code>, <code class="docutils literal"><span class="pre">pfft_execute</span></code> becomes
<code class="docutils literal"><span class="pre">pfftf_execute</span></code>, etcetera.)</p>
<p>Uppercase names, i.e. names beginning with ‘<code class="docutils literal"><span class="pre">PFFT_</span></code>’, remain the same.</p>
<p>Replace <code class="docutils literal"><span class="pre">double</span></code> with <code class="docutils literal"><span class="pre">float</span></code> or <code class="docutils literal"><span class="pre">long</span> <span class="pre">double</span></code> for subroutine
parameters.</p>
</div>
<div class="section" id="ghost-cell-communication">
<h2>Ghost cell communication<a class="headerlink" href="#ghost-cell-communication" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="fortran-interface">
<h2>Fortran interface<a class="headerlink" href="#fortran-interface" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Tutorial</a><ul>
<li><a class="reference internal" href="#a-first-parallel-transform-three-dimensional-fft-with-two-dimensional-data-decomposition">A first parallel transform - Three-dimensional FFT with two-dimensional data decomposition</a></li>
<li><a class="reference internal" href="#porting-fftw-mpi-based-code-to-pfft">Porting FFTW-MPI based code to PFFT</a></li>
<li><a class="reference internal" href="#errorcode-for-communicator-creation">Errorcode for communicator creation</a></li>
<li><a class="reference internal" href="#inplace-transforms">Inplace transforms</a></li>
<li><a class="reference internal" href="#higher-dimensional-data-decomposition">Higher dimensional data decomposition</a></li>
<li><a class="reference internal" href="#parallel-data-decomposition">Parallel data decomposition</a><ul>
<li><a class="reference internal" href="#non-transposed-and-transposed-data-layout">Non-transposed and transposed data layout</a></li>
<li><a class="reference internal" href="#three-dimensional-ffts-with-three-dimensional-data-decomposition">Three-dimensional FFTs with three-dimensional data decomposition</a></li>
</ul>
</li>
<li><a class="reference internal" href="#planning-effort">Planning effort</a></li>
<li><a class="reference internal" href="#preserving-input-data">Preserving input data</a></li>
<li><a class="reference internal" href="#ffts-with-shifted-index-sets">FFTs with shifted index sets</a></li>
<li><a class="reference internal" href="#pruned-fft-and-shifted-index-sets">Pruned FFT and Shifted Index Sets</a><ul>
<li><a class="reference internal" href="#pruned-fft">Pruned FFT</a></li>
<li><a class="reference internal" href="#shifted-index-sets">Shifted Index Sets</a></li>
</ul>
</li>
<li><a class="reference internal" href="#precisions">Precisions</a></li>
<li><a class="reference internal" href="#ghost-cell-communication">Ghost cell communication</a></li>
<li><a class="reference internal" href="#fortran-interface">Fortran interface</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="intro.html"
                        title="previous chapter">Introduction</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="install.html"
                        title="next chapter">Installation and linking</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/tutorial.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="install.html" title="Installation and linking"
             >next</a> |</li>
        <li class="right" >
          <a href="intro.html" title="Introduction"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">PFFT 1.0.8 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, Michael Pippig.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>